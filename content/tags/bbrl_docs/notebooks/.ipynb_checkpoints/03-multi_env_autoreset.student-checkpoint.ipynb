{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7db349d",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "This notebook is designed to understand how to use a gymnasium environment as a BBRL agent in practice, using autoreset=True.\n",
    "It is part of the [BBRL documentation](https://github.com/osigaud/bbrl/docs/index.html).\n",
    "\n",
    "If this is your first contact with BBRL, you may start be having a look at [this more basic notebook](01-basic_concepts.student.ipynb) and [the one using autoreset=False](02-multi_env_noautoreset.student.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0f148c",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "\n",
    "The BBRL library is [here](https://github.com/osigaud/bbrl).\n",
    "\n",
    "Below, we import standard python packages, pytorch packages and gymnasium environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a3b800",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[easypip] Installing bbrl_gymnasium>=0.2.0\n",
      "[easypip] Installing bbrl_gymnasium[classic_control]\n"
     ]
    }
   ],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc447d2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "\n",
    "import copy\n",
    "from abc import abstractmethod, ABC\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import strftime\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"current_time\", lambda: strftime(\"%Y%m%d-%H%M%S\"), replace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a63142",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports all the necessary classes and functions from BBRL\n",
    "from bbrl.agents.agent import Agent\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "# The workspace is the main class in BBRL, this is where all data is collected and stored\n",
    "from bbrl.workspace import Workspace\n",
    "\n",
    "# Agents(agent1, agent2, agent3, ...) executes the different agents the one after the other\n",
    "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n",
    "# or until a given condition is reached\n",
    "\n",
    "from bbrl.agents import Agents, TemporalAgent\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "\n",
    "# Replay buffers are useful to store past transitions when training\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac37db",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Definition of agents\n",
    "\n",
    "We reuse the RandomAgent already used in the autoreset=False case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52448c79",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, t: int, choose_action=True, **kwargs):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = torch.randint(0, self.action_dim, (len(obs), ))\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d8ea5",
   "metadata": {},
   "source": [
    "As before, we create an Agent representing [the CartPole-v1 gym environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "This is done using the [ParallelGymAgent](https://github.com/osigaud/bbrl/blob/40fe0468feb8998e62c3cd6bb3a575fef88e256f/src/bbrl/agents/gymnasium.py#L261) class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b3712",
   "metadata": {},
   "source": [
    "### Single environment case\n",
    "\n",
    "We start with a single instance of the CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57063923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: observation space in R^4 and action space R^2\n"
     ]
    }
   ],
   "source": [
    "# We deal with 1 environment (random seed 2139)\n",
    "\n",
    "env_agent = ParallelGymAgent(partial(make_env, env_name='CartPole-v1', autoreset=True), num_envs=1).seed(2139)\n",
    "obs_size, action_dim = env_agent.get_obs_and_actions_sizes()\n",
    "print(f\"Environment: observation space in R^{obs_size} and action space R^{action_dim}\")\n",
    "\n",
    "# Each agent is run in the order given when constructing Agents\n",
    "\n",
    "agents = Agents(env_agent, RandomAgent(action_dim))\n",
    "t_agents = TemporalAgent(agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd392e30",
   "metadata": {},
   "source": [
    "Let us have a closer look at the content of the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a74f4fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Observations (first 4)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0471,  0.0265,  0.0220, -0.0336]],\n",
       "\n",
       "        [[-0.0466, -0.1689,  0.0214,  0.2660]],\n",
       "\n",
       "        [[-0.0500, -0.3643,  0.0267,  0.5653]],\n",
       "\n",
       "        [[-0.0572, -0.5598,  0.0380,  0.8663]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Transitions (first 3)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'(s_0, s_1)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0471,  0.0265,  0.0220, -0.0336],\n",
       "        [-0.0466, -0.1689,  0.0214,  0.2660]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'(s_1, s_2)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0466, -0.1689,  0.0214,  0.2660],\n",
       "        [-0.0500, -0.3643,  0.0267,  0.5653]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'(s_2, s_3)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0500, -0.3643,  0.0267,  0.5653],\n",
       "        [-0.0572, -0.5598,  0.0380,  0.8663]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creates a new workspace\n",
    "workspace = Workspace() \n",
    "epoch_size = 6\n",
    "t_agents(workspace, n_steps=epoch_size)\n",
    "\n",
    "# We get the transitions: each tensor is transformed so that: \n",
    "# - we have the value at time step t and t+1 (so all the tensors first dimension have a size of 2)\n",
    "# - there is no distinction between the different environments (here, there is just one environment to make it easy)\n",
    "transitions = workspace.get_transitions()\n",
    "\n",
    "display(\"Observations (first 4)\", workspace[\"env/env_obs\"][:4])\n",
    "\n",
    "display(\"Transitions (first 3)\")\n",
    "for t in range(3):\n",
    "    display(f'(s_{t}, s_{t+1})')\n",
    "    # We ignore the first dimension as it corresponds to [t, t+1]\n",
    "    display(transitions[\"env/env_obs\"][:, t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca6ec8",
   "metadata": {},
   "source": [
    "You can see that each transition in the workspace corresponds to a pair of observations.\n",
    "\n",
    "### Transitions as a workspace\n",
    "\n",
    "A transition workspace is still a workspace... this is quite\n",
    " handy since each transition can be seen as a mini-episode of two time steps;\n",
    " we can use our agents on it.\n",
    "\n",
    "It is often the case in BBRL that we have to apply an agent to an already existing workspace\n",
    "as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93d34ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env/env_obs tensor([[[-0.0471,  0.0265,  0.0220, -0.0336],\n",
      "         [-0.0466, -0.1689,  0.0214,  0.2660],\n",
      "         [-0.0500, -0.3643,  0.0267,  0.5653],\n",
      "         [-0.0572, -0.5598,  0.0380,  0.8663],\n",
      "         [-0.0684, -0.3652,  0.0553,  0.5858]],\n",
      "\n",
      "        [[-0.0466, -0.1689,  0.0214,  0.2660],\n",
      "         [-0.0500, -0.3643,  0.0267,  0.5653],\n",
      "         [-0.0572, -0.5598,  0.0380,  0.8663],\n",
      "         [-0.0684, -0.3652,  0.0553,  0.5858],\n",
      "         [-0.0757, -0.1709,  0.0670,  0.3111]]])\n",
      "env/terminated tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n",
      "env/truncated tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n",
      "env/done tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n",
      "env/reward tensor([[0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "env/cumulated_reward tensor([[0., 1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4., 5.]])\n",
      "env/timestep tensor([[0, 1, 2, 3, 4],\n",
      "        [1, 2, 3, 4, 5]])\n",
      "action tensor([[0, 0, 0, 1, 1],\n",
      "        [0, 0, 1, 1, 0]])\n",
      "new action, tensor([[0, 0, 1, 1, 1],\n",
      "        [0, 1, 0, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "for key in transitions.variables.keys():\n",
    "    print(key, transitions[key])\n",
    "\n",
    "t_random_agent = TemporalAgent(RandomAgent(action_dim))\n",
    "t_random_agent(transitions, t=0, n_steps=2)\n",
    "\n",
    "# Here, the action tensor will have been overwritten by the new actions\n",
    "print(f\"new action, {transitions['action']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a3b1b",
   "metadata": {},
   "source": [
    "### Multiple environment case\n",
    "\n",
    "Now we are using 3 environments.\n",
    "Given the organization of transitions, to find the transitions of a particular environment\n",
    "we have to watch in the transition every 3 lines, since transitions are stored one environment after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2160cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: observation space in R^4 and action space R^2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Observations (first 4)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.5048e-03, -4.2718e-02, -4.8940e-02,  2.1523e-02],\n",
       "         [ 5.4922e-04,  2.4692e-03, -4.9253e-02, -4.0183e-02],\n",
       "         [ 8.0318e-03,  2.0348e-02, -2.2937e-03, -8.5254e-03]],\n",
       "\n",
       "        [[-9.3592e-03, -2.3711e-01, -4.8510e-02,  2.9837e-01],\n",
       "         [ 5.9860e-04, -1.9191e-01, -5.0056e-02,  2.3656e-01],\n",
       "         [ 8.4387e-03,  2.1550e-01, -2.4643e-03, -3.0193e-01]],\n",
       "\n",
       "        [[-1.4101e-02, -4.3150e-01, -4.2542e-02,  5.7537e-01],\n",
       "         [-3.2397e-03, -3.8629e-01, -4.5325e-02,  5.1305e-01],\n",
       "         [ 1.2749e-02,  2.0416e-02, -8.5029e-03, -1.0026e-02]],\n",
       "\n",
       "        [[-2.2731e-02, -6.2600e-01, -3.1035e-02,  8.5435e-01],\n",
       "         [-1.0965e-02, -5.8074e-01, -3.5064e-02,  7.9111e-01],\n",
       "         [ 1.3157e-02, -1.7458e-01, -8.7034e-03,  2.7996e-01]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Transitions (first 3)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'(s_0, s_1)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0085, -0.0427, -0.0489,  0.0215],\n",
       "        [-0.0094, -0.2371, -0.0485,  0.2984]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'(s_1, s_2)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0005,  0.0025, -0.0493, -0.0402],\n",
       "        [ 0.0006, -0.1919, -0.0501,  0.2366]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'(s_2, s_3)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0080,  0.0203, -0.0023, -0.0085],\n",
       "        [ 0.0084,  0.2155, -0.0025, -0.3019]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We deal with 3 environments at a time (random seed 2139)\n",
    "\n",
    "multienv_agent = ParallelGymAgent(partial(make_env, env_name='CartPole-v1', autoreset=True), num_envs=3).seed(2139)\n",
    "obs_size, action_dim = multienv_agent.get_obs_and_actions_sizes()\n",
    "print(f\"Environment: observation space in R^{obs_size} and action space R^{action_dim}\")\n",
    "\n",
    "agents = Agents(multienv_agent, RandomAgent(action_dim))\n",
    "t_agents = TemporalAgent(agents)\n",
    "workspace = Workspace() \n",
    "t_agents(workspace, n_steps=epoch_size)\n",
    "transitions = workspace.get_transitions()\n",
    "\n",
    "display(\"Observations (first 4)\", workspace[\"env/env_obs\"][:4])\n",
    "\n",
    "display(\"Transitions (first 3)\")\n",
    "for t in range(3):\n",
    "    display(f'(s_{t}, s_{t+1})')\n",
    "    display(transitions[\"env/env_obs\"][:, t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485deeaa",
   "metadata": {},
   "source": [
    "You can see how the transitions are organized in the workspace relative to the 3 environments.\n",
    "You first get the first transition from the first environment.\n",
    "Then the first transition from the second environment.\n",
    "Then the first transition from the third environment.\n",
    "Then the second transition from the first environment, etc.\n",
    "\n",
    "## The replay buffer\n",
    "\n",
    "Differently from the previous case, we use a replace buffer that stores\n",
    "a set of transitions $(s_t, a_t, r_t, s_{t+1})$\n",
    "Finally, the replay buffer keeps slices [:, i, ...] of the transition\n",
    "workspace (here at most 80 transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e06b0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0097,  0.0207, -0.0031, -0.0155],\n",
       "         [-0.0227, -0.6260, -0.0310,  0.8544],\n",
       "         [ 0.0080,  0.0203, -0.0023, -0.0085]],\n",
       "\n",
       "        [[ 0.0101, -0.1744, -0.0034,  0.2762],\n",
       "         [-0.0353, -0.8207, -0.0139,  1.1371],\n",
       "         [ 0.0084,  0.2155, -0.0025, -0.3019]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb = ReplayBuffer(max_size=80)\n",
    "\n",
    "# We add the transitions to the buffer....\n",
    "rb.put(transitions)\n",
    "\n",
    "# And sample from them here we get 3 tuples (s_t, s_{t+1})\n",
    "rb.get_shuffled(3)[\"env/env_obs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364318c5",
   "metadata": {},
   "source": [
    "## Collecting several epochs into the same workspace\n",
    "\n",
    "In the code below, the workspace only contains one epoch at a time.\n",
    "The content of these different epochs are concatenated into the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b34b630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting new epoch, already performed 27 steps\n",
      "replay buffer size: 42\n",
      "collecting new epoch, already performed 56 steps\n",
      "replay buffer size: 71\n",
      "collecting new epoch, already performed 85 steps\n",
      "replay buffer size: 80\n",
      "collecting new epoch, already performed 113 steps\n",
      "replay buffer size: 80\n"
     ]
    }
   ],
   "source": [
    "nb_steps = 0\n",
    "max_steps = 100\n",
    "epoch_size = 10\n",
    "\n",
    "while nb_steps < max_steps:\n",
    "    # Execute the agent in the workspace\n",
    "    if nb_steps == 0:\n",
    "        # In the first epoch, we start with t=0\n",
    "        t_agents(workspace, t=0, n_steps=epoch_size)\n",
    "    else:\n",
    "        # Clear all gradient graphs from the workspace\n",
    "        workspace.zero_grad()\n",
    "        # Here we duplicate the last column of the previous epoch into the first column of the next epoch\n",
    "        workspace.copy_n_last_steps(1)\n",
    "\n",
    "        # In subsequent epochs, we start with t=1 so as to avoid overwriting the first column we just duplicated\n",
    "        t_agents(workspace, t=1, n_steps=epoch_size)\n",
    "\n",
    "    transition_workspace = workspace.get_transitions()\n",
    "\n",
    "    # The part below counts the number of steps: it ignores action performed during transition from one episode to the next,\n",
    "    # as they have been discarded by the get_transitions() function\n",
    "\n",
    "    action = transition_workspace[\"action\"]\n",
    "    nb_steps += action[0].shape[0]\n",
    "    print(f\"collecting new epoch, already performed {nb_steps} steps\")\n",
    "\n",
    "    if nb_steps > 0 or epoch_size  > 1:\n",
    "        rb.put(transition_workspace)\n",
    "    print(f\"replay buffer size: {rb.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb550995",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a stupid agent that always outputs action 1, run it for 10 epochs of 100 steps over 2 instances of the CartPole-v1 environment.\n",
    "Put the data into a replay buffer of size 5000.\n",
    "Count the number of episodes the agent performed in each environment in several ways:\n",
    "- by counting the number of \"done=True\" elements in the workspace before applying the `get_transitions()` function\n",
    "- by measuring the difference between the size of the replay buffer and the number of steps performed by the agents."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
